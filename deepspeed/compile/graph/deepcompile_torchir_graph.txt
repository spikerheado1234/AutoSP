def forward(self, L_kwargs_input_ids_ : torch.Tensor, L_args_0_modules_model_modules_embed_tokens_parameters_weight_ : torch.nn.parameter.Parameter, L_kwargs_attention_mask_ : torch.Tensor, L_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_ : torch.Tensor, L_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ : torch.nn.parameter.Parameter, L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_args_0_modules_model_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ : torch.nn.parameter.Parameter, L_args_0_modules_model_modules_layers_modules_0_modules_mlp_modules_gate_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_args_0_modules_model_modules_layers_modules_0_modules_mlp_modules_up_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_args_0_modules_model_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ : torch.nn.parameter.Parameter, L_args_0_modules_model_modules_norm_parameters_weight_ : torch.nn.parameter.Parameter, L_args_0_modules_lm_head_parameters_weight_ : torch.nn.parameter.Parameter, L_kwargs_labels_ : torch.Tensor):
    l_kwargs_input_ids_ = L_kwargs_input_ids_
    l_args_0_modules_model_modules_embed_tokens_parameters_weight_ = L_args_0_modules_model_modules_embed_tokens_parameters_weight_
    l_kwargs_attention_mask_ = L_kwargs_attention_mask_
    l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_
    l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
    l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
    l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
    l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
    l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_
    l_args_0_modules_model_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_
    l_args_0_modules_model_modules_layers_modules_0_modules_mlp_modules_gate_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_mlp_modules_gate_proj_parameters_weight_
    l_args_0_modules_model_modules_layers_modules_0_modules_mlp_modules_up_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_mlp_modules_up_proj_parameters_weight_
    l_args_0_modules_model_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_
    l_args_0_modules_model_modules_norm_parameters_weight_ = L_args_0_modules_model_modules_norm_parameters_weight_
    l_args_0_modules_lm_head_parameters_weight_ = L_args_0_modules_lm_head_parameters_weight_
    l_kwargs_labels_ = L_kwargs_labels_
    inputs_embeds = torch.nn.functional.embedding(l_kwargs_input_ids_, l_args_0_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_kwargs_input_ids_ = l_args_0_modules_model_modules_embed_tokens_parameters_weight_ = None
    cache_position = torch.arange(0, 1024, device = device(type='cuda', index=0))
    position_ids = cache_position.unsqueeze(0);  cache_position = None
    causal_mask = torch.full((2048, 2048), fill_value = -3.3895313892515355e+38, dtype = torch.bfloat16, device = device(type='cuda', index=0))
    causal_mask_1 = torch.triu(causal_mask, diagonal = 1);  causal_mask = None
    arange_1 = torch.arange(2048, device = device(type='cuda', index=0))
    arange_2 = torch.arange(0, 2048, device = device(type='cuda', index=0))
    reshape = arange_2.reshape(-1, 1);  arange_2 = None
    gt = arange_1 > reshape;  arange_1 = reshape = None
    causal_mask_1 *= gt;  causal_mask_2 = causal_mask_1;  causal_mask_1 = gt = None
    getitem = causal_mask_2[(None, None, slice(None, None, None), slice(None, None, None))];  causal_mask_2 = None
    causal_mask_3 = getitem.expand(1, 1, -1, -1);  getitem = None
    causal_mask_4 = causal_mask_3.clone();  causal_mask_3 = None
    getitem_1 = causal_mask_4[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
    getitem_2 = l_kwargs_attention_mask_[(slice(None, None, None), None, None, slice(None, None, None))];  l_kwargs_attention_mask_ = None
    to = getitem_2.to(device(type='cuda', index=0));  getitem_2 = None
    padding_mask = getitem_1 + to;  getitem_1 = to = None
    padding_mask_1 = padding_mask == 0;  padding_mask = None
    getitem_3 = causal_mask_4[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))]
    masked_fill = getitem_3.masked_fill(padding_mask_1, -3.3895313892515355e+38);  getitem_3 = padding_mask_1 = None
    causal_mask_4[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))] = masked_fill;  setitem = causal_mask_4;  masked_fill = setitem = None
    eq_1 = causal_mask_4 == -3.3895313892515355e+38
    all_1 = torch.all(eq_1, dim = -1, keepdim = True);  eq_1 = None
    invert = ~all_1;  all_1 = None
    causal_mask_5 = causal_mask_4.mul(invert);  causal_mask_4 = invert = None
    _set_grad_enabled = torch._C._set_grad_enabled(False);  _set_grad_enabled = None
    getitem_4 = l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
    float_1 = getitem_4.float();  getitem_4 = None
    inv_freq_expanded = float_1.expand(1, -1, 1);  float_1 = None
    getitem_5 = position_ids[(slice(None, None, None), None, slice(None, None, None))];  position_ids = None
    position_ids_expanded = getitem_5.float();  getitem_5 = None
    _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
    float_3 = inv_freq_expanded.float();  inv_freq_expanded = None
    to_1 = float_3.to(device(type='cuda', index=0));  float_3 = None
    float_4 = position_ids_expanded.float();  position_ids_expanded = None
    matmul = to_1 @ float_4;  to_1 = float_4 = None
    freqs = matmul.transpose(1, 2);  matmul = None
    emb = torch.cat((freqs, freqs), dim = -1);  freqs = None
    cos = emb.cos()
    sin = emb.sin();  emb = None
    _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
    cos_1 = cos * 1.0;  cos = None
    sin_1 = sin * 1.0;  sin = None
    cos_2 = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
    sin_2 = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
    _set_grad_enabled_1 = torch._C._set_grad_enabled(True);  _set_grad_enabled_1 = None
    _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
    hidden_states = inputs_embeds.to(torch.float32)
    pow_1 = hidden_states.pow(2)
    variance = pow_1.mean(-1, keepdim = True);  pow_1 = None
    add_1 = variance + 1e-05;  variance = None
    rsqrt = torch.rsqrt(add_1);  add_1 = None
    hidden_states_1 = hidden_states * rsqrt;  hidden_states = rsqrt = None
    to_5 = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
    hidden_states_2 = l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_5;  l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_5 = None
    linear = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
    view = linear.view((1, 1024, -1, 128));  linear = None
    query_states = view.transpose(1, 2);  view = None
    linear_1 = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
    view_1 = linear_1.view((1, 1024, -1, 128));  linear_1 = None
    key_states = view_1.transpose(1, 2);  view_1 = None
    linear_2 = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
    view_2 = linear_2.view((1, 1024, -1, 128));  linear_2 = None
    value_states = view_2.transpose(1, 2);  view_2 = None
    cos_3 = cos_2.unsqueeze(1);  cos_2 = None
    sin_3 = sin_2.unsqueeze(1);  sin_2 = None
    mul_5 = query_states * cos_3
    x1 = query_states[(Ellipsis, slice(None, 64, None))]
    x2 = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
    neg = -x2;  x2 = None
    cat_1 = torch.cat((neg, x1), dim = -1);  neg = x1 = None
    mul_6 = cat_1 * sin_3;  cat_1 = None
    q_embed = mul_5 + mul_6;  mul_5 = mul_6 = None
    mul_7 = key_states * cos_3;  cos_3 = None
    x1_1 = key_states[(Ellipsis, slice(None, 64, None))]
    x2_1 = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
    neg_1 = -x2_1;  x2_1 = None
    cat_2 = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
    mul_8 = cat_2 * sin_3;  cat_2 = sin_3 = None
    k_embed = mul_7 + mul_8;  mul_7 = mul_8 = None
    causal_mask_6 = causal_mask_5[(slice(None, None, None), slice(None, None, None), slice(None, None, None), slice(None, 2048, None))];  causal_mask_5 = None
    query = q_embed.contiguous();  q_embed = None
    all_to_all_qkv_default = torch.ops.ulysses.all_to_all_qkv.default(query, 1, 2048, 32, 128, 2);  query = None
    key = k_embed.contiguous();  k_embed = None
    all_to_all_qkv_default_1 = torch.ops.ulysses.all_to_all_qkv.default(key, 1, 2048, 32, 128, 2);  key = None
    value = value_states.contiguous();  value_states = None
    all_to_all_qkv_default_2 = torch.ops.ulysses.all_to_all_qkv.default(value, 1, 2048, 32, 128, 2);  value = None
    attn_output = torch._C._nn.scaled_dot_product_attention(all_to_all_qkv_default, all_to_all_qkv_default_1, all_to_all_qkv_default_2, attn_mask = causal_mask_6, dropout_p = 0.0, scale = 0.08838834764831845, is_causal = False);  all_to_all_qkv_default = all_to_all_qkv_default_1 = all_to_all_qkv_default_2 = causal_mask_6 = None
    all_to_all_out_default = torch.ops.ulysses.all_to_all_out.default(attn_output, 1, 2048, 32, 128, 2);  attn_output = None
    transpose_4 = all_to_all_out_default.transpose(1, 2);  all_to_all_out_default = None
    attn_output_1 = transpose_4.contiguous();  transpose_4 = None
    reshape_1 = attn_output_1.reshape(1, 1024, -1);  attn_output_1 = None
    attn_output_2 = reshape_1.contiguous();  reshape_1 = None
    attn_output_3 = torch._C._nn.linear(attn_output_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_, None);  attn_output_2 = l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_o_proj_parameters_weight_ = None
    hidden_states_3 = inputs_embeds + attn_output_3;  inputs_embeds = attn_output_3 = None
    hidden_states_4 = hidden_states_3.to(torch.float32)
    pow_2 = hidden_states_4.pow(2)
    variance_1 = pow_2.mean(-1, keepdim = True);  pow_2 = None
    add_5 = variance_1 + 1e-05;  variance_1 = None
    rsqrt_1 = torch.rsqrt(add_5);  add_5 = None
    hidden_states_5 = hidden_states_4 * rsqrt_1;  hidden_states_4 = rsqrt_1 = None
    to_7 = hidden_states_5.to(torch.bfloat16);  hidden_states_5 = None
    hidden_states_6 = l_args_0_modules_model_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ * to_7;  l_args_0_modules_model_modules_layers_modules_0_modules_post_attention_layernorm_parameters_weight_ = to_7 = None
    linear_4 = torch._C._nn.linear(hidden_states_6, l_args_0_modules_model_modules_layers_modules_0_modules_mlp_modules_gate_proj_parameters_weight_, None);  l_args_0_modules_model_modules_layers_modules_0_modules_mlp_modules_gate_proj_parameters_weight_ = None
    silu = torch.nn.functional.silu(linear_4, inplace = False);  linear_4 = None
    linear_5 = torch._C._nn.linear(hidden_states_6, l_args_0_modules_model_modules_layers_modules_0_modules_mlp_modules_up_proj_parameters_weight_, None);  hidden_states_6 = l_args_0_modules_model_modules_layers_modules_0_modules_mlp_modules_up_proj_parameters_weight_ = None
    mul_11 = silu * linear_5;  silu = linear_5 = None
    down_proj = torch._C._nn.linear(mul_11, l_args_0_modules_model_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_, None);  mul_11 = l_args_0_modules_model_modules_layers_modules_0_modules_mlp_modules_down_proj_parameters_weight_ = None
    hidden_states_7 = hidden_states_3 + down_proj;  hidden_states_3 = down_proj = None
    hidden_states_8 = hidden_states_7.to(torch.float32);  hidden_states_7 = None
    pow_3 = hidden_states_8.pow(2)
    variance_2 = pow_3.mean(-1, keepdim = True);  pow_3 = None
    add_7 = variance_2 + 1e-05;  variance_2 = None
    rsqrt_2 = torch.rsqrt(add_7);  add_7 = None
    hidden_states_9 = hidden_states_8 * rsqrt_2;  hidden_states_8 = rsqrt_2 = None
    to_9 = hidden_states_9.to(torch.bfloat16);  hidden_states_9 = None
    hidden_states_10 = l_args_0_modules_model_modules_norm_parameters_weight_ * to_9;  l_args_0_modules_model_modules_norm_parameters_weight_ = to_9 = None
    getitem_11 = hidden_states_10[(slice(None, None, None), slice(0, None, None), slice(None, None, None))];  hidden_states_10 = None
    logits = torch._C._nn.linear(getitem_11, l_args_0_modules_lm_head_parameters_weight_, None);  getitem_11 = l_args_0_modules_lm_head_parameters_weight_ = None
    logits_1 = logits.float()
    labels = l_kwargs_labels_.to(device(type='cuda', index=0));  l_kwargs_labels_ = None
    labels_1 = torch._C._nn.pad(labels, (0, 1), 'constant', -100);  labels = None
    getitem_12 = labels_1[(Ellipsis, slice(1, None, None))];  labels_1 = None
    shift_labels = getitem_12.contiguous();  getitem_12 = None
    logits_2 = logits_1.view(-1, 32000);  logits_1 = None
    shift_labels_1 = shift_labels.view(-1);  shift_labels = None
    shift_labels_2 = shift_labels_1.to(device(type='cuda', index=0));  shift_labels_1 = None
    loss = torch.nn.functional.cross_entropy(logits_2, shift_labels_2, ignore_index = -100, reduction = 'mean');  logits_2 = shift_labels_2 = None
    return (loss, logits)
    
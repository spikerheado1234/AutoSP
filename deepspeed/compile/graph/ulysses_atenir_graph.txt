class joint_helper(torch.nn.Module):
    def forward(self, primals, tangents):
        primals_1: "bf16[1, 2048, 4096]"; primals_2: "bf16[1, 2048, 128]"; primals_3: "bf16[1, 2048, 128]"; primals_4: "bf16[4096]"; tangents_1: "bf16[1, 1024, 4096]"; 
    
        primals_1, primals_2, primals_3, primals_4, tangents_1, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1555 in forward, code: hidden_states = hidden_states[:, start:end, :]
        slice_1: "bf16[1, 2048, 4096]" = torch.ops.aten.slice.Tensor(primals_1, 0, 0, 9223372036854775807);  primals_1 = None
        slice_2: "bf16[1, 1024, 4096]" = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 1024);  slice_1 = None
        slice_3: "bf16[1, 1024, 4096]" = torch.ops.aten.slice.Tensor(slice_2, 2, 0, 9223372036854775807);  slice_2 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1558 in <genexpr>, code: pe[:, start:end, :] if pe.shape[1] > seqs_per_rank else pe
        slice_4: "bf16[1, 2048, 128]" = torch.ops.aten.slice.Tensor(primals_2, 0, 0, 9223372036854775807);  primals_2 = None
        slice_5: "bf16[1, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_4, 1, 0, 1024);  slice_4 = None
        slice_6: "bf16[1, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_5, 2, 0, 9223372036854775807);  slice_5 = None
        slice_7: "bf16[1, 2048, 128]" = torch.ops.aten.slice.Tensor(primals_3, 0, 0, 9223372036854775807);  primals_3 = None
        slice_8: "bf16[1, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_7, 1, 0, 1024);  slice_7 = None
        slice_9: "bf16[1, 1024, 128]" = torch.ops.aten.slice.Tensor(slice_8, 2, 0, 9223372036854775807);  slice_8 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1256 in forward, code: hidden_states = hidden_states.to(torch.float32)
        convert_element_type: "f32[1, 1024, 4096]" = torch.ops.prims.convert_element_type.default(slice_3, torch.float32)
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1257 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 1024, 4096]" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 2)
        mean: "f32[1, 1024, 1]" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1258 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 1024, 1]" = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
        rsqrt: "f32[1, 1024, 1]" = torch.ops.aten.rsqrt.default(add);  add = None
        alias: "f32[1, 1024, 1]" = torch.ops.aten.alias.default(rsqrt)
        alias_1: "f32[1, 1024, 1]" = torch.ops.aten.alias.default(alias);  alias = None
        mul: "f32[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type, rsqrt)
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1259 in forward, code: return self.weight * hidden_states.to(input_dtype)
        convert_element_type_1: "bf16[1, 1024, 4096]" = torch.ops.prims.convert_element_type.default(mul, torch.bfloat16);  mul = None
        mul_1: "bf16[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(primals_4, convert_element_type_1)
        mul_2: "bf16[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(tangents_1, primals_4);  primals_4 = None
        mul_3: "bf16[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(tangents_1, convert_element_type_1);  tangents_1 = convert_element_type_1 = None
        sum_1: "bf16[1, 1, 4096]" = torch.ops.aten.sum.dim_IntList(mul_3, [0, 1], True);  mul_3 = None
        view: "bf16[4096]" = torch.ops.aten.view.default(sum_1, [4096]);  sum_1 = None
        convert_element_type_2: "f32[1, 1024, 4096]" = torch.ops.prims.convert_element_type.default(mul_2, torch.float32);  mul_2 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1258 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        mul_4: "f32[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_2, convert_element_type)
        mul_5: "f32[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_2, rsqrt);  convert_element_type_2 = rsqrt = None
        sum_2: "f32[1, 1024, 1]" = torch.ops.aten.sum.dim_IntList(mul_4, [2], True);  mul_4 = None
        alias_2: "f32[1, 1024, 1]" = torch.ops.aten.alias.default(alias_1);  alias_1 = None
        alias_3: "f32[1, 1024, 1]" = torch.ops.aten.alias.default(alias_2);  alias_2 = None
        pow_2: "f32[1, 1024, 1]" = torch.ops.aten.pow.Tensor_Scalar(alias_3, 3);  alias_3 = None
        mul_6: "f32[1, 1024, 1]" = torch.ops.aten.mul.Scalar(sum_2, -0.5);  sum_2 = None
        mul_7: "f32[1, 1024, 1]" = torch.ops.aten.mul.Tensor(mul_6, pow_2);  mul_6 = pow_2 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1257 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        expand: "f32[1, 1024, 4096]" = torch.ops.aten.expand.default(mul_7, [1, 1024, 4096]);  mul_7 = None
        div: "f32[1, 1024, 4096]" = torch.ops.aten.div.Scalar(expand, 4096);  expand = None
        pow_3: "f32[1, 1024, 4096]" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 1.0);  convert_element_type = None
        mul_8: "f32[1, 1024, 4096]" = torch.ops.aten.mul.Scalar(pow_3, 2.0);  pow_3 = None
        mul_9: "f32[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(div, mul_8);  div = mul_8 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1257 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        add_1: "f32[1, 1024, 4096]" = torch.ops.aten.add.Tensor(mul_5, mul_9);  mul_5 = mul_9 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1256 in forward, code: hidden_states = hidden_states.to(torch.float32)
        convert_element_type_3: "bf16[1, 1024, 4096]" = torch.ops.prims.convert_element_type.default(add_1, torch.bfloat16);  add_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1555 in forward, code: hidden_states = hidden_states[:, start:end, :]
        full: "bf16[1, 1024, 4096]" = torch.ops.aten.full.default([1, 1024, 4096], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        slice_scatter: "bf16[1, 1024, 4096]" = torch.ops.aten.slice_scatter.default(full, convert_element_type_3, 2, 0, 9223372036854775807);  full = convert_element_type_3 = None
        full_1: "bf16[1, 2048, 4096]" = torch.ops.aten.full.default([1, 2048, 4096], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        slice_scatter_1: "bf16[1, 2048, 4096]" = torch.ops.aten.slice_scatter.default(full_1, slice_scatter, 1, 0, 1024);  full_1 = slice_scatter = None
        full_2: "bf16[1, 2048, 4096]" = torch.ops.aten.full.default([1, 2048, 4096], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        slice_scatter_2: "bf16[1, 2048, 4096]" = torch.ops.aten.slice_scatter.default(full_2, slice_scatter_1, 0, 0, 9223372036854775807);  full_2 = slice_scatter_1 = None
        return pytree.tree_unflatten([mul_1, slice_6, slice_9, slice_3, slice_scatter_2, None, None, view], self._out_spec)
        
class joint_helper(torch.nn.Module):
    def forward(self, primals, tangents):
        primals_1: "bf16[1, 1024, 4096]"; primals_2: "bf16[4096, 4096]"; primals_3: "bf16[4096, 4096]"; primals_4: "bf16[4096, 4096]"; primals_5: "bf16[1, 1024, 128]"; primals_6: "bf16[1, 1024, 128]"; tangents_1: "bf16[1, 1024, 32, 128]"; tangents_2: "bf16[1, 1024, 32, 128]"; tangents_3: "bf16[1, 1024, 32, 128]"; 
    
        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, tangents_1, tangents_2, tangents_3, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1496 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute: "bf16[4096, 4096]" = torch.ops.aten.permute.default(primals_2, [1, 0]);  primals_2 = None
        view: "bf16[1024, 4096]" = torch.ops.aten.view.default(primals_1, [1024, 4096])
        mm: "bf16[1024, 4096]" = torch.ops.aten.mm.default(view, permute)
        view_1: "bf16[1, 1024, 4096]" = torch.ops.aten.view.default(mm, [1, 1024, 4096]);  mm = None
        view_2: "bf16[1, 1024, 32, 128]" = torch.ops.aten.view.default(view_1, [1, 1024, -1, 128]);  view_1 = None
        permute_1: "bf16[1, 32, 1024, 128]" = torch.ops.aten.permute.default(view_2, [0, 2, 1, 3]);  view_2 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1497 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_2: "bf16[4096, 4096]" = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None
        view_3: "bf16[1024, 4096]" = torch.ops.aten.view.default(primals_1, [1024, 4096])
        mm_1: "bf16[1024, 4096]" = torch.ops.aten.mm.default(view_3, permute_2)
        view_4: "bf16[1, 1024, 4096]" = torch.ops.aten.view.default(mm_1, [1, 1024, 4096]);  mm_1 = None
        view_5: "bf16[1, 1024, 32, 128]" = torch.ops.aten.view.default(view_4, [1, 1024, -1, 128]);  view_4 = None
        permute_3: "bf16[1, 32, 1024, 128]" = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1498 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_4: "bf16[4096, 4096]" = torch.ops.aten.permute.default(primals_4, [1, 0]);  primals_4 = None
        view_6: "bf16[1024, 4096]" = torch.ops.aten.view.default(primals_1, [1024, 4096]);  primals_1 = None
        mm_2: "bf16[1024, 4096]" = torch.ops.aten.mm.default(view_6, permute_4)
        view_7: "bf16[1, 1024, 4096]" = torch.ops.aten.view.default(mm_2, [1, 1024, 4096]);  mm_2 = None
        view_8: "bf16[1, 1024, 32, 128]" = torch.ops.aten.view.default(view_7, [1, 1024, -1, 128]);  view_7 = None
        permute_5: "bf16[1, 32, 1024, 128]" = torch.ops.aten.permute.default(view_8, [0, 2, 1, 3]);  view_8 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1356 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        unsqueeze: "bf16[1, 1, 1024, 128]" = torch.ops.aten.unsqueeze.default(primals_5, 1);  primals_5 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1357 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        unsqueeze_1: "bf16[1, 1, 1024, 128]" = torch.ops.aten.unsqueeze.default(primals_6, 1);  primals_6 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1358 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul: "bf16[1, 32, 1024, 128]" = torch.ops.aten.mul.Tensor(permute_1, unsqueeze)
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1331 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_1: "bf16[1, 32, 1024, 64]" = torch.ops.aten.slice.Tensor(permute_1, 3, 0, 64)
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1332 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_2: "bf16[1, 32, 1024, 64]" = torch.ops.aten.slice.Tensor(permute_1, 3, 64, 9223372036854775807);  permute_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1333 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 1024, 64]" = torch.ops.aten.neg.default(slice_2);  slice_2 = None
        cat: "bf16[1, 32, 1024, 128]" = torch.ops.aten.cat.default([neg, slice_1], -1);  neg = slice_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1358 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_1: "bf16[1, 32, 1024, 128]" = torch.ops.aten.mul.Tensor(cat, unsqueeze_1);  cat = None
        add: "bf16[1, 32, 1024, 128]" = torch.ops.aten.add.Tensor(mul, mul_1);  mul = mul_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1359 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_2: "bf16[1, 32, 1024, 128]" = torch.ops.aten.mul.Tensor(permute_3, unsqueeze)
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1331 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        slice_3: "bf16[1, 32, 1024, 64]" = torch.ops.aten.slice.Tensor(permute_3, 3, 0, 64)
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1332 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        slice_4: "bf16[1, 32, 1024, 64]" = torch.ops.aten.slice.Tensor(permute_3, 3, 64, 9223372036854775807);  permute_3 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1333 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 32, 1024, 64]" = torch.ops.aten.neg.default(slice_4);  slice_4 = None
        cat_1: "bf16[1, 32, 1024, 128]" = torch.ops.aten.cat.default([neg_1, slice_3], -1);  neg_1 = slice_3 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1359 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_3: "bf16[1, 32, 1024, 128]" = torch.ops.aten.mul.Tensor(cat_1, unsqueeze_1);  cat_1 = None
        add_1: "bf16[1, 32, 1024, 128]" = torch.ops.aten.add.Tensor(mul_2, mul_3);  mul_2 = mul_3 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1509 in forward, code: query_states = query_states.transpose(1, 2)
        permute_6: "bf16[1, 1024, 32, 128]" = torch.ops.aten.permute.default(add, [0, 2, 1, 3]);  add = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1510 in forward, code: key_states = key_states.transpose(1, 2)
        permute_7: "bf16[1, 1024, 32, 128]" = torch.ops.aten.permute.default(add_1, [0, 2, 1, 3]);  add_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1511 in forward, code: value_states = value_states.transpose(1, 2)
        permute_8: "bf16[1, 1024, 32, 128]" = torch.ops.aten.permute.default(permute_5, [0, 2, 1, 3]);  permute_5 = None
        permute_9: "bf16[1, 32, 1024, 128]" = torch.ops.aten.permute.default(tangents_3, [0, 2, 1, 3]);  tangents_3 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1510 in forward, code: key_states = key_states.transpose(1, 2)
        permute_10: "bf16[1, 32, 1024, 128]" = torch.ops.aten.permute.default(tangents_2, [0, 2, 1, 3]);  tangents_2 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1509 in forward, code: query_states = query_states.transpose(1, 2)
        permute_11: "bf16[1, 32, 1024, 128]" = torch.ops.aten.permute.default(tangents_1, [0, 2, 1, 3]);  tangents_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1359 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_4: "bf16[1, 32, 1024, 128]" = torch.ops.aten.mul.Tensor(permute_10, unsqueeze_1)
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1333 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        slice_5: "bf16[1, 32, 1024, 64]" = torch.ops.aten.slice.Tensor(mul_4, 3, 0, 64)
        slice_6: "bf16[1, 32, 1024, 64]" = torch.ops.aten.slice.Tensor(mul_4, 3, 64, 128);  mul_4 = None
        neg_2: "bf16[1, 32, 1024, 64]" = torch.ops.aten.neg.default(slice_5);  slice_5 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1332 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        full: "bf16[1, 32, 1024, 128]" = torch.ops.aten.full.default([1, 32, 1024, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        slice_scatter: "bf16[1, 32, 1024, 128]" = torch.ops.aten.slice_scatter.default(full, neg_2, 3, 64, 9223372036854775807);  full = neg_2 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1331 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        full_1: "bf16[1, 32, 1024, 128]" = torch.ops.aten.full.default([1, 32, 1024, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        slice_scatter_1: "bf16[1, 32, 1024, 128]" = torch.ops.aten.slice_scatter.default(full_1, slice_6, 3, 0, 64);  full_1 = slice_6 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1331 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        add_2: "bf16[1, 32, 1024, 128]" = torch.ops.aten.add.Tensor(slice_scatter, slice_scatter_1);  slice_scatter = slice_scatter_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1359 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_5: "bf16[1, 32, 1024, 128]" = torch.ops.aten.mul.Tensor(permute_10, unsqueeze);  permute_10 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1359 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        add_3: "bf16[1, 32, 1024, 128]" = torch.ops.aten.add.Tensor(add_2, mul_5);  add_2 = mul_5 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1358 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_6: "bf16[1, 32, 1024, 128]" = torch.ops.aten.mul.Tensor(permute_11, unsqueeze_1);  unsqueeze_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1333 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        slice_7: "bf16[1, 32, 1024, 64]" = torch.ops.aten.slice.Tensor(mul_6, 3, 0, 64)
        slice_8: "bf16[1, 32, 1024, 64]" = torch.ops.aten.slice.Tensor(mul_6, 3, 64, 128);  mul_6 = None
        neg_3: "bf16[1, 32, 1024, 64]" = torch.ops.aten.neg.default(slice_7);  slice_7 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1332 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        full_2: "bf16[1, 32, 1024, 128]" = torch.ops.aten.full.default([1, 32, 1024, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        slice_scatter_2: "bf16[1, 32, 1024, 128]" = torch.ops.aten.slice_scatter.default(full_2, neg_3, 3, 64, 9223372036854775807);  full_2 = neg_3 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1331 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        full_3: "bf16[1, 32, 1024, 128]" = torch.ops.aten.full.default([1, 32, 1024, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        slice_scatter_3: "bf16[1, 32, 1024, 128]" = torch.ops.aten.slice_scatter.default(full_3, slice_8, 3, 0, 64);  full_3 = slice_8 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1331 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        add_4: "bf16[1, 32, 1024, 128]" = torch.ops.aten.add.Tensor(slice_scatter_2, slice_scatter_3);  slice_scatter_2 = slice_scatter_3 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1358 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_7: "bf16[1, 32, 1024, 128]" = torch.ops.aten.mul.Tensor(permute_11, unsqueeze);  permute_11 = unsqueeze = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1358 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        add_5: "bf16[1, 32, 1024, 128]" = torch.ops.aten.add.Tensor(add_4, mul_7);  add_4 = mul_7 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1498 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_12: "bf16[1, 1024, 32, 128]" = torch.ops.aten.permute.default(permute_9, [0, 2, 1, 3]);  permute_9 = None
        view_9: "bf16[1, 1024, 4096]" = torch.ops.aten.view.default(permute_12, [1, 1024, 4096]);  permute_12 = None
        view_10: "bf16[1024, 4096]" = torch.ops.aten.view.default(view_9, [1024, 4096]);  view_9 = None
        permute_13: "bf16[4096, 1024]" = torch.ops.aten.permute.default(view_10, [1, 0])
        mm_3: "bf16[4096, 4096]" = torch.ops.aten.mm.default(permute_13, view_6);  permute_13 = view_6 = None
        permute_14: "bf16[4096, 4096]" = torch.ops.aten.permute.default(mm_3, [1, 0]);  mm_3 = None
        permute_15: "bf16[4096, 4096]" = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None
        mm_4: "bf16[1024, 4096]" = torch.ops.aten.mm.default(view_10, permute_15);  view_10 = permute_15 = None
        view_11: "bf16[1, 1024, 4096]" = torch.ops.aten.view.default(mm_4, [1, 1024, 4096]);  mm_4 = None
        permute_16: "bf16[4096, 4096]" = torch.ops.aten.permute.default(permute_14, [1, 0]);  permute_14 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1497 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_17: "bf16[1, 1024, 32, 128]" = torch.ops.aten.permute.default(add_3, [0, 2, 1, 3]);  add_3 = None
        clone: "bf16[1, 1024, 32, 128]" = torch.ops.aten.clone.default(permute_17, memory_format = torch.contiguous_format);  permute_17 = None
        view_12: "bf16[1, 1024, 4096]" = torch.ops.aten.view.default(clone, [1, 1024, 4096]);  clone = None
        view_13: "bf16[1024, 4096]" = torch.ops.aten.view.default(view_12, [1024, 4096]);  view_12 = None
        permute_18: "bf16[4096, 1024]" = torch.ops.aten.permute.default(view_13, [1, 0])
        mm_5: "bf16[4096, 4096]" = torch.ops.aten.mm.default(permute_18, view_3);  permute_18 = view_3 = None
        permute_19: "bf16[4096, 4096]" = torch.ops.aten.permute.default(mm_5, [1, 0]);  mm_5 = None
        permute_20: "bf16[4096, 4096]" = torch.ops.aten.permute.default(permute_2, [1, 0]);  permute_2 = None
        mm_6: "bf16[1024, 4096]" = torch.ops.aten.mm.default(view_13, permute_20);  view_13 = permute_20 = None
        view_14: "bf16[1, 1024, 4096]" = torch.ops.aten.view.default(mm_6, [1, 1024, 4096]);  mm_6 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1497 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        add_6: "bf16[1, 1024, 4096]" = torch.ops.aten.add.Tensor(view_11, view_14);  view_11 = view_14 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1497 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_21: "bf16[4096, 4096]" = torch.ops.aten.permute.default(permute_19, [1, 0]);  permute_19 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1496 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_22: "bf16[1, 1024, 32, 128]" = torch.ops.aten.permute.default(add_5, [0, 2, 1, 3]);  add_5 = None
        clone_1: "bf16[1, 1024, 32, 128]" = torch.ops.aten.clone.default(permute_22, memory_format = torch.contiguous_format);  permute_22 = None
        view_15: "bf16[1, 1024, 4096]" = torch.ops.aten.view.default(clone_1, [1, 1024, 4096]);  clone_1 = None
        view_16: "bf16[1024, 4096]" = torch.ops.aten.view.default(view_15, [1024, 4096]);  view_15 = None
        permute_23: "bf16[4096, 1024]" = torch.ops.aten.permute.default(view_16, [1, 0])
        mm_7: "bf16[4096, 4096]" = torch.ops.aten.mm.default(permute_23, view);  permute_23 = view = None
        permute_24: "bf16[4096, 4096]" = torch.ops.aten.permute.default(mm_7, [1, 0]);  mm_7 = None
        permute_25: "bf16[4096, 4096]" = torch.ops.aten.permute.default(permute, [1, 0]);  permute = None
        mm_8: "bf16[1024, 4096]" = torch.ops.aten.mm.default(view_16, permute_25);  view_16 = permute_25 = None
        view_17: "bf16[1, 1024, 4096]" = torch.ops.aten.view.default(mm_8, [1, 1024, 4096]);  mm_8 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1496 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        add_7: "bf16[1, 1024, 4096]" = torch.ops.aten.add.Tensor(add_6, view_17);  add_6 = view_17 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1496 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        permute_26: "bf16[4096, 4096]" = torch.ops.aten.permute.default(permute_24, [1, 0]);  permute_24 = None
        return pytree.tree_unflatten([permute_6, permute_7, permute_8, add_7, permute_26, permute_21, permute_16, None, None], self._out_spec)
        
/u/zwang22/miniconda3/envs/dc/lib/python3.12/site-packages/torch/autograd/function.py:575: UserWarning: Detected non-contiguous tensor in P2P operations. It is user responsibility to guarantee that source and destination tensors have the same contiguity format. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2386.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
/u/zwang22/miniconda3/envs/dc/lib/python3.12/site-packages/torch/autograd/function.py:575: UserWarning: Detected non-contiguous tensor in P2P operations. It is user responsibility to guarantee that source and destination tensors have the same contiguity format. (Triggered internally at ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2386.)
  return super().apply(*args, **kwargs)  # type: ignore[misc]
class joint_helper(torch.nn.Module):
    def forward(self, primals, tangents):
        primals_1: "bf16[1, 2048, 16, 128]"; primals_2: "bf16[1, 2048, 16, 128]"; primals_3: "bf16[1, 2048, 16, 128]"; primals_4: "bf16[1, 1, 2048, 2048]"; tangents_1: "bf16[1, 2048, 16, 128]"; tangents_2: "bf16[1, 16, 2048, 2048]"; 
    
        primals_1, primals_2, primals_3, primals_4, tangents_1, tangents_2, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1404 in eager_attention_forward, code: query = query.transpose(1, 2)
        permute: "bf16[1, 16, 2048, 128]" = torch.ops.aten.permute.default(primals_2, [0, 2, 1, 3]);  primals_2 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1405 in eager_attention_forward, code: key = key.transpose(1, 2)
        permute_1: "bf16[1, 16, 2048, 128]" = torch.ops.aten.permute.default(primals_3, [0, 2, 1, 3]);  primals_3 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1406 in eager_attention_forward, code: value = value.transpose(1, 2)
        permute_2: "bf16[1, 16, 2048, 128]" = torch.ops.aten.permute.default(primals_1, [0, 2, 1, 3]);  primals_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1410 in eager_attention_forward, code: attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
        permute_3: "bf16[1, 16, 128, 2048]" = torch.ops.aten.permute.default(permute_1, [0, 1, 3, 2]);  permute_1 = None
        expand: "bf16[1, 16, 2048, 128]" = torch.ops.aten.expand.default(permute, [1, 16, 2048, 128]);  permute = None
        view: "bf16[16, 2048, 128]" = torch.ops.aten.view.default(expand, [16, 2048, 128]);  expand = None
        expand_1: "bf16[1, 16, 128, 2048]" = torch.ops.aten.expand.default(permute_3, [1, 16, 128, 2048]);  permute_3 = None
        view_1: "bf16[16, 128, 2048]" = torch.ops.aten.view.default(expand_1, [16, 128, 2048]);  expand_1 = None
        bmm: "bf16[16, 2048, 2048]" = torch.ops.aten.bmm.default(view, view_1)
        view_2: "bf16[1, 16, 2048, 2048]" = torch.ops.aten.view.default(bmm, [1, 16, 2048, 2048]);  bmm = None
        mul: "bf16[1, 16, 2048, 2048]" = torch.ops.aten.mul.Tensor(view_2, 0.08838834764831845);  view_2 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1412 in eager_attention_forward, code: causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
        slice_1: "bf16[1, 1, 2048, 2048]" = torch.ops.aten.slice.Tensor(primals_4, 0, 0, 9223372036854775807);  primals_4 = None
        slice_2: "bf16[1, 1, 2048, 2048]" = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 9223372036854775807);  slice_1 = None
        slice_3: "bf16[1, 1, 2048, 2048]" = torch.ops.aten.slice.Tensor(slice_2, 2, 0, 9223372036854775807);  slice_2 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1413 in eager_attention_forward, code: attn_weights = attn_weights + causal_mask
        add: "bf16[1, 16, 2048, 2048]" = torch.ops.aten.add.Tensor(mul, slice_3);  mul = slice_3 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1415 in eager_attention_forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
        convert_element_type_2: "f32[1, 16, 2048, 2048]" = torch.ops.prims.convert_element_type.default(add, torch.float32);  add = None
        amax: "f32[1, 16, 2048, 1]" = torch.ops.aten.amax.default(convert_element_type_2, [-1], True)
        sub: "f32[1, 16, 2048, 2048]" = torch.ops.aten.sub.Tensor(convert_element_type_2, amax);  convert_element_type_2 = amax = None
        exp: "f32[1, 16, 2048, 2048]" = torch.ops.aten.exp.default(sub);  sub = None
        sum_1: "f32[1, 16, 2048, 1]" = torch.ops.aten.sum.dim_IntList(exp, [-1], True)
        div: "f32[1, 16, 2048, 2048]" = torch.ops.aten.div.Tensor(exp, sum_1);  exp = sum_1 = None
        alias: "f32[1, 16, 2048, 2048]" = torch.ops.aten.alias.default(div)
        alias_1: "f32[1, 16, 2048, 2048]" = torch.ops.aten.alias.default(alias);  alias = None
        convert_element_type_3: "bf16[1, 16, 2048, 2048]" = torch.ops.prims.convert_element_type.default(div, torch.bfloat16);  div = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1416 in eager_attention_forward, code: attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=training)
        clone: "bf16[1, 16, 2048, 2048]" = torch.ops.aten.clone.default(convert_element_type_3);  convert_element_type_3 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1417 in eager_attention_forward, code: attn_output = torch.matmul(attn_weights, value_states)
        expand_2: "bf16[1, 16, 2048, 2048]" = torch.ops.aten.expand.default(clone, [1, 16, 2048, 2048])
        view_3: "bf16[16, 2048, 2048]" = torch.ops.aten.view.default(expand_2, [16, 2048, 2048]);  expand_2 = None
        expand_3: "bf16[1, 16, 2048, 128]" = torch.ops.aten.expand.default(permute_2, [1, 16, 2048, 128]);  permute_2 = None
        view_4: "bf16[16, 2048, 128]" = torch.ops.aten.view.default(expand_3, [16, 2048, 128]);  expand_3 = None
        bmm_1: "bf16[16, 2048, 128]" = torch.ops.aten.bmm.default(view_3, view_4)
        view_5: "bf16[1, 16, 2048, 128]" = torch.ops.aten.view.default(bmm_1, [1, 16, 2048, 128]);  bmm_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1418 in eager_attention_forward, code: attn_output = attn_output.transpose(1, 2).contiguous()
        permute_4: "bf16[1, 2048, 16, 128]" = torch.ops.aten.permute.default(view_5, [0, 2, 1, 3]);  view_5 = None
        clone_1: "bf16[1, 2048, 16, 128]" = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None
        permute_5: "bf16[1, 16, 2048, 128]" = torch.ops.aten.permute.default(tangents_1, [0, 2, 1, 3]);  tangents_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1417 in eager_attention_forward, code: attn_output = torch.matmul(attn_weights, value_states)
        view_6: "bf16[16, 2048, 128]" = torch.ops.aten.view.default(permute_5, [16, 2048, 128]);  permute_5 = None
        permute_6: "bf16[16, 2048, 2048]" = torch.ops.aten.permute.default(view_3, [0, 2, 1]);  view_3 = None
        bmm_2: "bf16[16, 2048, 128]" = torch.ops.aten.bmm.default(permute_6, view_6);  permute_6 = None
        permute_7: "bf16[16, 128, 2048]" = torch.ops.aten.permute.default(view_4, [0, 2, 1]);  view_4 = None
        bmm_3: "bf16[16, 2048, 2048]" = torch.ops.aten.bmm.default(view_6, permute_7);  view_6 = permute_7 = None
        view_7: "bf16[1, 16, 2048, 128]" = torch.ops.aten.view.default(bmm_2, [1, 16, 2048, 128]);  bmm_2 = None
        view_8: "bf16[1, 16, 2048, 2048]" = torch.ops.aten.view.default(bmm_3, [1, 16, 2048, 2048]);  bmm_3 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1417 in eager_attention_forward, code: attn_output = torch.matmul(attn_weights, value_states)
        add_1: "bf16[1, 16, 2048, 2048]" = torch.ops.aten.add.Tensor(tangents_2, view_8);  tangents_2 = view_8 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1415 in eager_attention_forward, code: attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)
        convert_element_type_10: "f32[1, 16, 2048, 2048]" = torch.ops.prims.convert_element_type.default(add_1, torch.float32);  add_1 = None
        alias_2: "f32[1, 16, 2048, 2048]" = torch.ops.aten.alias.default(alias_1);  alias_1 = None
        alias_3: "f32[1, 16, 2048, 2048]" = torch.ops.aten.alias.default(alias_2);  alias_2 = None
        mul_1: "f32[1, 16, 2048, 2048]" = torch.ops.aten.mul.Tensor(convert_element_type_10, alias_3);  convert_element_type_10 = None
        sum_2: "f32[1, 16, 2048, 1]" = torch.ops.aten.sum.dim_IntList(mul_1, [-1], True)
        neg: "f32[1, 16, 2048, 2048]" = torch.ops.aten.neg.default(alias_3);  alias_3 = None
        fma: "f32[1, 16, 2048, 2048]" = torch.ops.prims.fma.default(neg, sum_2, mul_1);  neg = sum_2 = mul_1 = None
        convert_element_type_11: "bf16[1, 16, 2048, 2048]" = torch.ops.prims.convert_element_type.default(fma, torch.bfloat16);  fma = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1410 in eager_attention_forward, code: attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling
        mul_2: "bf16[1, 16, 2048, 2048]" = torch.ops.aten.mul.Tensor(convert_element_type_11, 0.08838834764831845);  convert_element_type_11 = None
        view_9: "bf16[16, 2048, 2048]" = torch.ops.aten.view.default(mul_2, [16, 2048, 2048]);  mul_2 = None
        permute_8: "bf16[16, 128, 2048]" = torch.ops.aten.permute.default(view, [0, 2, 1]);  view = None
        bmm_4: "bf16[16, 128, 2048]" = torch.ops.aten.bmm.default(permute_8, view_9);  permute_8 = None
        permute_9: "bf16[16, 2048, 128]" = torch.ops.aten.permute.default(view_1, [0, 2, 1]);  view_1 = None
        bmm_5: "bf16[16, 2048, 128]" = torch.ops.aten.bmm.default(view_9, permute_9);  view_9 = permute_9 = None
        view_10: "bf16[1, 16, 128, 2048]" = torch.ops.aten.view.default(bmm_4, [1, 16, 128, 2048]);  bmm_4 = None
        view_11: "bf16[1, 16, 2048, 128]" = torch.ops.aten.view.default(bmm_5, [1, 16, 2048, 128]);  bmm_5 = None
        permute_10: "bf16[1, 16, 2048, 128]" = torch.ops.aten.permute.default(view_10, [0, 1, 3, 2]);  view_10 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1406 in eager_attention_forward, code: value = value.transpose(1, 2)
        permute_11: "bf16[1, 2048, 16, 128]" = torch.ops.aten.permute.default(view_7, [0, 2, 1, 3]);  view_7 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1405 in eager_attention_forward, code: key = key.transpose(1, 2)
        permute_12: "bf16[1, 2048, 16, 128]" = torch.ops.aten.permute.default(permute_10, [0, 2, 1, 3]);  permute_10 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1404 in eager_attention_forward, code: query = query.transpose(1, 2)
        permute_13: "bf16[1, 2048, 16, 128]" = torch.ops.aten.permute.default(view_11, [0, 2, 1, 3]);  view_11 = None
        return pytree.tree_unflatten([clone_1, clone, permute_11, permute_13, permute_12, None], self._out_spec)
        
class joint_helper(torch.nn.Module):
    def forward(self, primals, tangents):
        primals_1: "bf16[1, 1024, 32, 128]"; primals_2: "bf16[4096, 4096]"; tangents_1: "bf16[1, 1024, 4096]"; 
    
        primals_1, primals_2, tangents_1, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1521 in torch_dynamo_resume_in_forward_at_1513, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view: "bf16[1, 1024, 4096]" = torch.ops.aten.view.default(primals_1, [1, 1024, -1]);  primals_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1522 in torch_dynamo_resume_in_forward_at_1513, code: attn_output = self.o_proj(attn_output)
        permute: "bf16[4096, 4096]" = torch.ops.aten.permute.default(primals_2, [1, 0]);  primals_2 = None
        view_1: "bf16[1024, 4096]" = torch.ops.aten.view.default(view, [1024, 4096]);  view = None
        mm: "bf16[1024, 4096]" = torch.ops.aten.mm.default(view_1, permute)
        view_2: "bf16[1, 1024, 4096]" = torch.ops.aten.view.default(mm, [1, 1024, 4096]);  mm = None
        view_3: "bf16[1024, 4096]" = torch.ops.aten.view.default(tangents_1, [1024, 4096]);  tangents_1 = None
        permute_1: "bf16[4096, 1024]" = torch.ops.aten.permute.default(view_3, [1, 0])
        mm_1: "bf16[4096, 4096]" = torch.ops.aten.mm.default(permute_1, view_1);  permute_1 = view_1 = None
        permute_2: "bf16[4096, 4096]" = torch.ops.aten.permute.default(mm_1, [1, 0]);  mm_1 = None
        permute_3: "bf16[4096, 4096]" = torch.ops.aten.permute.default(permute, [1, 0]);  permute = None
        mm_2: "bf16[1024, 4096]" = torch.ops.aten.mm.default(view_3, permute_3);  view_3 = permute_3 = None
        view_4: "bf16[1, 1024, 4096]" = torch.ops.aten.view.default(mm_2, [1, 1024, 4096]);  mm_2 = None
        permute_4: "bf16[4096, 4096]" = torch.ops.aten.permute.default(permute_2, [1, 0]);  permute_2 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1521 in torch_dynamo_resume_in_forward_at_1513, code: attn_output = attn_output.reshape(*input_shape, -1).contiguous()
        view_5: "bf16[1, 1024, 32, 128]" = torch.ops.aten.view.default(view_4, [1, 1024, 32, 128]);  view_4 = None
        return pytree.tree_unflatten([view_2, view_5, permute_4], self._out_spec)
        
class joint_helper(torch.nn.Module):
    def forward(self, primals, tangents):
        primals_1: "bf16[1, 1024, 4096]"; primals_2: "bf16[1, 1024, 4096]"; primals_3: "bf16[4096]"; primals_4: "bf16[11008, 4096]"; primals_5: "bf16[11008, 4096]"; primals_6: "bf16[4096, 11008]"; tangents_1: "bf16[1, 1024, 4096]"; 
    
        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, tangents_1, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1578 in torch_dynamo_resume_in_forward_at_1567, code: hidden_states = residual + hidden_states
        add: "bf16[1, 1024, 4096]" = torch.ops.aten.add.Tensor(primals_2, primals_1);  primals_2 = primals_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1256 in forward, code: hidden_states = hidden_states.to(torch.float32)
        convert_element_type: "f32[1, 1024, 4096]" = torch.ops.prims.convert_element_type.default(add, torch.float32)
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1257 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 1024, 4096]" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 2)
        mean: "f32[1, 1024, 1]" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1258 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add_1: "f32[1, 1024, 1]" = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
        rsqrt: "f32[1, 1024, 1]" = torch.ops.aten.rsqrt.default(add_1);  add_1 = None
        alias: "f32[1, 1024, 1]" = torch.ops.aten.alias.default(rsqrt)
        alias_1: "f32[1, 1024, 1]" = torch.ops.aten.alias.default(alias);  alias = None
        mul: "f32[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type, rsqrt)
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1259 in forward, code: return self.weight * hidden_states.to(input_dtype)
        convert_element_type_1: "bf16[1, 1024, 4096]" = torch.ops.prims.convert_element_type.default(mul, torch.bfloat16);  mul = None
        mul_1: "bf16[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(primals_3, convert_element_type_1)
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1375 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute: "bf16[4096, 11008]" = torch.ops.aten.permute.default(primals_4, [1, 0]);  primals_4 = None
        view: "bf16[1024, 4096]" = torch.ops.aten.view.default(mul_1, [1024, 4096])
        mm: "bf16[1024, 11008]" = torch.ops.aten.mm.default(view, permute)
        view_1: "bf16[1, 1024, 11008]" = torch.ops.aten.view.default(mm, [1, 1024, 11008]);  mm = None
        convert_element_type_4: "f32[1, 1024, 11008]" = torch.ops.prims.convert_element_type.default(view_1, torch.float32)
        sigmoid: "f32[1, 1024, 11008]" = torch.ops.aten.sigmoid.default(convert_element_type_4)
        mul_2: "f32[1, 1024, 11008]" = torch.ops.aten.mul.Tensor(convert_element_type_4, sigmoid);  convert_element_type_4 = sigmoid = None
        convert_element_type_5: "bf16[1, 1024, 11008]" = torch.ops.prims.convert_element_type.default(mul_2, torch.bfloat16);  mul_2 = None
        permute_1: "bf16[4096, 11008]" = torch.ops.aten.permute.default(primals_5, [1, 0]);  primals_5 = None
        view_2: "bf16[1024, 4096]" = torch.ops.aten.view.default(mul_1, [1024, 4096]);  mul_1 = None
        mm_1: "bf16[1024, 11008]" = torch.ops.aten.mm.default(view_2, permute_1)
        view_3: "bf16[1, 1024, 11008]" = torch.ops.aten.view.default(mm_1, [1, 1024, 11008]);  mm_1 = None
        mul_3: "bf16[1, 1024, 11008]" = torch.ops.aten.mul.Tensor(convert_element_type_5, view_3)
        permute_2: "bf16[11008, 4096]" = torch.ops.aten.permute.default(primals_6, [1, 0]);  primals_6 = None
        view_4: "bf16[1024, 11008]" = torch.ops.aten.view.default(mul_3, [1024, 11008]);  mul_3 = None
        mm_2: "bf16[1024, 4096]" = torch.ops.aten.mm.default(view_4, permute_2)
        view_5: "bf16[1, 1024, 4096]" = torch.ops.aten.view.default(mm_2, [1, 1024, 4096]);  mm_2 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1584 in torch_dynamo_resume_in_forward_at_1567, code: hidden_states = residual + hidden_states
        add_2: "bf16[1, 1024, 4096]" = torch.ops.aten.add.Tensor(add, view_5);  add = view_5 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1375 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        view_6: "bf16[1024, 4096]" = torch.ops.aten.view.default(tangents_1, [1024, 4096])
        permute_3: "bf16[4096, 1024]" = torch.ops.aten.permute.default(view_6, [1, 0])
        mm_3: "bf16[4096, 11008]" = torch.ops.aten.mm.default(permute_3, view_4);  permute_3 = view_4 = None
        permute_4: "bf16[11008, 4096]" = torch.ops.aten.permute.default(mm_3, [1, 0]);  mm_3 = None
        permute_5: "bf16[4096, 11008]" = torch.ops.aten.permute.default(permute_2, [1, 0]);  permute_2 = None
        mm_4: "bf16[1024, 11008]" = torch.ops.aten.mm.default(view_6, permute_5);  view_6 = permute_5 = None
        view_7: "bf16[1, 1024, 11008]" = torch.ops.aten.view.default(mm_4, [1, 1024, 11008]);  mm_4 = None
        permute_6: "bf16[4096, 11008]" = torch.ops.aten.permute.default(permute_4, [1, 0]);  permute_4 = None
        mul_4: "bf16[1, 1024, 11008]" = torch.ops.aten.mul.Tensor(view_7, convert_element_type_5);  convert_element_type_5 = None
        mul_5: "bf16[1, 1024, 11008]" = torch.ops.aten.mul.Tensor(view_7, view_3);  view_7 = view_3 = None
        view_8: "bf16[1024, 11008]" = torch.ops.aten.view.default(mul_4, [1024, 11008]);  mul_4 = None
        permute_7: "bf16[11008, 1024]" = torch.ops.aten.permute.default(view_8, [1, 0])
        mm_5: "bf16[11008, 4096]" = torch.ops.aten.mm.default(permute_7, view_2);  permute_7 = view_2 = None
        permute_8: "bf16[4096, 11008]" = torch.ops.aten.permute.default(mm_5, [1, 0]);  mm_5 = None
        permute_9: "bf16[11008, 4096]" = torch.ops.aten.permute.default(permute_1, [1, 0]);  permute_1 = None
        mm_6: "bf16[1024, 4096]" = torch.ops.aten.mm.default(view_8, permute_9);  view_8 = permute_9 = None
        view_9: "bf16[1, 1024, 4096]" = torch.ops.aten.view.default(mm_6, [1, 1024, 4096]);  mm_6 = None
        permute_10: "bf16[11008, 4096]" = torch.ops.aten.permute.default(permute_8, [1, 0]);  permute_8 = None
        sigmoid_1: "bf16[1, 1024, 11008]" = torch.ops.aten.sigmoid.default(view_1)
        full: "bf16[1, 1024, 11008]" = torch.ops.aten.full.default([1, 1024, 11008], 1, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        sub: "bf16[1, 1024, 11008]" = torch.ops.aten.sub.Tensor(full, sigmoid_1);  full = None
        mul_6: "bf16[1, 1024, 11008]" = torch.ops.aten.mul.Tensor(view_1, sub);  view_1 = sub = None
        add_3: "bf16[1, 1024, 11008]" = torch.ops.aten.add.Scalar(mul_6, 1);  mul_6 = None
        mul_7: "bf16[1, 1024, 11008]" = torch.ops.aten.mul.Tensor(sigmoid_1, add_3);  sigmoid_1 = add_3 = None
        mul_8: "bf16[1, 1024, 11008]" = torch.ops.aten.mul.Tensor(mul_5, mul_7);  mul_5 = mul_7 = None
        view_10: "bf16[1024, 11008]" = torch.ops.aten.view.default(mul_8, [1024, 11008]);  mul_8 = None
        permute_12: "bf16[11008, 1024]" = torch.ops.aten.permute.default(view_10, [1, 0])
        mm_7: "bf16[11008, 4096]" = torch.ops.aten.mm.default(permute_12, view);  permute_12 = view = None
        permute_13: "bf16[4096, 11008]" = torch.ops.aten.permute.default(mm_7, [1, 0]);  mm_7 = None
        permute_14: "bf16[11008, 4096]" = torch.ops.aten.permute.default(permute, [1, 0]);  permute = None
        mm_8: "bf16[1024, 4096]" = torch.ops.aten.mm.default(view_10, permute_14);  view_10 = permute_14 = None
        view_11: "bf16[1, 1024, 4096]" = torch.ops.aten.view.default(mm_8, [1, 1024, 4096]);  mm_8 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1375 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        add_4: "bf16[1, 1024, 4096]" = torch.ops.aten.add.Tensor(view_9, view_11);  view_9 = view_11 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1375 in forward, code: down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
        permute_15: "bf16[11008, 4096]" = torch.ops.aten.permute.default(permute_13, [1, 0]);  permute_13 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1259 in forward, code: return self.weight * hidden_states.to(input_dtype)
        mul_9: "bf16[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(add_4, primals_3);  primals_3 = None
        mul_10: "bf16[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(add_4, convert_element_type_1);  add_4 = convert_element_type_1 = None
        sum_1: "bf16[1, 1, 4096]" = torch.ops.aten.sum.dim_IntList(mul_10, [0, 1], True);  mul_10 = None
        view_12: "bf16[4096]" = torch.ops.aten.view.default(sum_1, [4096]);  sum_1 = None
        convert_element_type_22: "f32[1, 1024, 4096]" = torch.ops.prims.convert_element_type.default(mul_9, torch.float32);  mul_9 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1258 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        mul_11: "f32[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_22, convert_element_type)
        mul_12: "f32[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_22, rsqrt);  convert_element_type_22 = rsqrt = None
        sum_2: "f32[1, 1024, 1]" = torch.ops.aten.sum.dim_IntList(mul_11, [2], True);  mul_11 = None
        alias_2: "f32[1, 1024, 1]" = torch.ops.aten.alias.default(alias_1);  alias_1 = None
        alias_3: "f32[1, 1024, 1]" = torch.ops.aten.alias.default(alias_2);  alias_2 = None
        pow_2: "f32[1, 1024, 1]" = torch.ops.aten.pow.Tensor_Scalar(alias_3, 3);  alias_3 = None
        mul_13: "f32[1, 1024, 1]" = torch.ops.aten.mul.Scalar(sum_2, -0.5);  sum_2 = None
        mul_14: "f32[1, 1024, 1]" = torch.ops.aten.mul.Tensor(mul_13, pow_2);  mul_13 = pow_2 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1257 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        expand: "f32[1, 1024, 4096]" = torch.ops.aten.expand.default(mul_14, [1, 1024, 4096]);  mul_14 = None
        div: "f32[1, 1024, 4096]" = torch.ops.aten.div.Scalar(expand, 4096);  expand = None
        pow_3: "f32[1, 1024, 4096]" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 1.0);  convert_element_type = None
        mul_15: "f32[1, 1024, 4096]" = torch.ops.aten.mul.Scalar(pow_3, 2.0);  pow_3 = None
        mul_16: "f32[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(div, mul_15);  div = mul_15 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1257 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        add_5: "f32[1, 1024, 4096]" = torch.ops.aten.add.Tensor(mul_12, mul_16);  mul_12 = mul_16 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1256 in forward, code: hidden_states = hidden_states.to(torch.float32)
        convert_element_type_23: "bf16[1, 1024, 4096]" = torch.ops.prims.convert_element_type.default(add_5, torch.bfloat16);  add_5 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1256 in forward, code: hidden_states = hidden_states.to(torch.float32)
        add_6: "bf16[1, 1024, 4096]" = torch.ops.aten.add.Tensor(tangents_1, convert_element_type_23);  tangents_1 = convert_element_type_23 = None
        return pytree.tree_unflatten([add_2, add_6, add_6, view_12, permute_15, permute_10, permute_6], self._out_spec)
        
class joint_helper(torch.nn.Module):
    def forward(self, primals, tangents):
        primals_1: "bf16[1, 1024, 4096]"; primals_2: "bf16[4096]"; tangents_1: "bf16[1, 1024, 4096]"; 
    
        primals_1, primals_2, tangents_1, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1256 in forward, code: hidden_states = hidden_states.to(torch.float32)
        convert_element_type: "f32[1, 1024, 4096]" = torch.ops.prims.convert_element_type.default(primals_1, torch.float32);  primals_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1257 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 1024, 4096]" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 2)
        mean: "f32[1, 1024, 1]" = torch.ops.aten.mean.dim(pow_1, [-1], True);  pow_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1258 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 1024, 1]" = torch.ops.aten.add.Tensor(mean, 1e-05);  mean = None
        rsqrt: "f32[1, 1024, 1]" = torch.ops.aten.rsqrt.default(add);  add = None
        alias: "f32[1, 1024, 1]" = torch.ops.aten.alias.default(rsqrt)
        alias_1: "f32[1, 1024, 1]" = torch.ops.aten.alias.default(alias);  alias = None
        mul: "f32[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type, rsqrt)
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1259 in forward, code: return self.weight * hidden_states.to(input_dtype)
        convert_element_type_1: "bf16[1, 1024, 4096]" = torch.ops.prims.convert_element_type.default(mul, torch.bfloat16);  mul = None
        mul_1: "bf16[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(primals_2, convert_element_type_1)
        mul_2: "bf16[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(tangents_1, primals_2);  primals_2 = None
        mul_3: "bf16[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(tangents_1, convert_element_type_1);  tangents_1 = convert_element_type_1 = None
        sum_1: "bf16[1, 1, 4096]" = torch.ops.aten.sum.dim_IntList(mul_3, [0, 1], True);  mul_3 = None
        view: "bf16[4096]" = torch.ops.aten.view.default(sum_1, [4096]);  sum_1 = None
        convert_element_type_2: "f32[1, 1024, 4096]" = torch.ops.prims.convert_element_type.default(mul_2, torch.float32);  mul_2 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1258 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        mul_4: "f32[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_2, convert_element_type)
        mul_5: "f32[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(convert_element_type_2, rsqrt);  convert_element_type_2 = rsqrt = None
        sum_2: "f32[1, 1024, 1]" = torch.ops.aten.sum.dim_IntList(mul_4, [2], True);  mul_4 = None
        alias_2: "f32[1, 1024, 1]" = torch.ops.aten.alias.default(alias_1);  alias_1 = None
        alias_3: "f32[1, 1024, 1]" = torch.ops.aten.alias.default(alias_2);  alias_2 = None
        pow_2: "f32[1, 1024, 1]" = torch.ops.aten.pow.Tensor_Scalar(alias_3, 3);  alias_3 = None
        mul_6: "f32[1, 1024, 1]" = torch.ops.aten.mul.Scalar(sum_2, -0.5);  sum_2 = None
        mul_7: "f32[1, 1024, 1]" = torch.ops.aten.mul.Tensor(mul_6, pow_2);  mul_6 = pow_2 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1257 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        expand: "f32[1, 1024, 4096]" = torch.ops.aten.expand.default(mul_7, [1, 1024, 4096]);  mul_7 = None
        div: "f32[1, 1024, 4096]" = torch.ops.aten.div.Scalar(expand, 4096);  expand = None
        pow_3: "f32[1, 1024, 4096]" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type, 1.0);  convert_element_type = None
        mul_8: "f32[1, 1024, 4096]" = torch.ops.aten.mul.Scalar(pow_3, 2.0);  pow_3 = None
        mul_9: "f32[1, 1024, 4096]" = torch.ops.aten.mul.Tensor(div, mul_8);  div = mul_8 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1257 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        add_1: "f32[1, 1024, 4096]" = torch.ops.aten.add.Tensor(mul_5, mul_9);  mul_5 = mul_9 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:1256 in forward, code: hidden_states = hidden_states.to(torch.float32)
        convert_element_type_3: "bf16[1, 1024, 4096]" = torch.ops.prims.convert_element_type.default(add_1, torch.bfloat16);  add_1 = None
        return pytree.tree_unflatten([mul_1, convert_element_type_3, view], self._out_spec)
        
class joint_helper(torch.nn.Module):
    def forward(self, primals, tangents):
        primals_1: "bf16[1, 1024, 4096]"; primals_2: "i64[1, 2048]"; primals_3: "bf16[32000, 4096]"; tangents_1: "f32[]"; tangents_2: "bf16[1, 1024, 32000]"; 
    
        primals_1, primals_2, primals_3, tangents_1, tangents_2, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:2115 in torch_dynamo_resume_in_forward_at_2094, code: labels = labels[:, start:end]
        slice_1: "i64[1, 2048]" = torch.ops.aten.slice.Tensor(primals_2, 0, 0, 9223372036854775807);  primals_2 = None
        slice_2: "i64[1, 1024]" = torch.ops.aten.slice.Tensor(slice_1, 1, 0, 1024);  slice_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:2120 in torch_dynamo_resume_in_forward_at_2094, code: logits = self.lm_head(hidden_states[:, slice_indices, :])
        slice_3: "bf16[1, 1024, 4096]" = torch.ops.aten.slice.Tensor(primals_1, 0, 0, 9223372036854775807);  primals_1 = None
        slice_4: "bf16[1, 1024, 4096]" = torch.ops.aten.slice.Tensor(slice_3, 1, 0, 9223372036854775807);  slice_3 = None
        slice_5: "bf16[1, 1024, 4096]" = torch.ops.aten.slice.Tensor(slice_4, 2, 0, 9223372036854775807);  slice_4 = None
        permute: "bf16[4096, 32000]" = torch.ops.aten.permute.default(primals_3, [1, 0]);  primals_3 = None
        view: "bf16[1024, 4096]" = torch.ops.aten.view.default(slice_5, [1024, 4096]);  slice_5 = None
        mm: "bf16[1024, 32000]" = torch.ops.aten.mm.default(view, permute)
        view_1: "bf16[1, 1024, 32000]" = torch.ops.aten.view.default(mm, [1, 1024, 32000]);  mm = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/loss/loss_utils.py:37 in ForCausalLMLoss, code: logits = logits.float()
        convert_element_type_2: "f32[1, 1024, 32000]" = torch.ops.prims.convert_element_type.default(view_1, torch.float32)
        
         # File: /u/zwang22/miniconda3/envs/dc/lib/python3.12/site-packages/torch/nn/functional.py:5096 in pad, code: return torch._C._nn.pad(input, pad, mode, value)
        constant_pad_nd: "i64[1, 1025]" = torch.ops.aten.constant_pad_nd.default(slice_2, [0, 1], -100.0);  slice_2 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/loss/loss_utils.py:41 in ForCausalLMLoss, code: shift_labels = labels[..., 1:].contiguous()
        slice_6: "i64[1, 1024]" = torch.ops.aten.slice.Tensor(constant_pad_nd, 1, 1, 9223372036854775807);  constant_pad_nd = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/loss/loss_utils.py:44 in ForCausalLMLoss, code: logits = logits.view(-1, vocab_size)
        view_2: "f32[1024, 32000]" = torch.ops.aten.view.default(convert_element_type_2, [-1, 32000]);  convert_element_type_2 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/loss/loss_utils.py:45 in ForCausalLMLoss, code: shift_labels = shift_labels.view(-1)
        view_3: "i64[1024]" = torch.ops.aten.view.default(slice_6, [-1]);  slice_6 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/loss/loss_utils.py:27 in fixed_cross_entropy, code: loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)
        amax: "f32[1024, 1]" = torch.ops.aten.amax.default(view_2, [1], True)
        sub: "f32[1024, 32000]" = torch.ops.aten.sub.Tensor(view_2, amax);  view_2 = amax = None
        exp: "f32[1024, 32000]" = torch.ops.aten.exp.default(sub)
        sum_1: "f32[1024, 1]" = torch.ops.aten.sum.dim_IntList(exp, [1], True);  exp = None
        log: "f32[1024, 1]" = torch.ops.aten.log.default(sum_1);  sum_1 = None
        sub_1: "f32[1024, 32000]" = torch.ops.aten.sub.Tensor(sub, log);  sub = log = None
        alias: "f32[1024, 32000]" = torch.ops.aten.alias.default(sub_1)
        alias_1: "f32[1024, 32000]" = torch.ops.aten.alias.default(alias);  alias = None
        ne: "b8[1024]" = torch.ops.aten.ne.Scalar(view_3, -100)
        scalar_tensor: "i64[]" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0))
        where: "i64[1024]" = torch.ops.aten.where.self(ne, view_3, scalar_tensor);  ne = scalar_tensor = None
        unsqueeze: "i64[1024, 1]" = torch.ops.aten.unsqueeze.default(where, 1);  where = None
        gather: "f32[1024, 1]" = torch.ops.aten.gather.default(sub_1, 1, unsqueeze);  sub_1 = unsqueeze = None
        squeeze: "f32[1024]" = torch.ops.aten.squeeze.dim(gather, 1);  gather = None
        neg: "f32[1024]" = torch.ops.aten.neg.default(squeeze);  squeeze = None
        ne_1: "b8[1024]" = torch.ops.aten.ne.Scalar(view_3, -100)
        scalar_tensor_1: "f32[]" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0))
        where_1: "f32[1024]" = torch.ops.aten.where.self(ne_1, neg, scalar_tensor_1);  ne_1 = neg = scalar_tensor_1 = None
        ne_2: "b8[1024]" = torch.ops.aten.ne.Scalar(view_3, -100)
        sum_2: "i64[]" = torch.ops.aten.sum.default(ne_2);  ne_2 = None
        convert_element_type_3: "f32[]" = torch.ops.prims.convert_element_type.default(sum_2, torch.float32);  sum_2 = None
        sum_3: "f32[]" = torch.ops.aten.sum.default(where_1);  where_1 = None
        div: "f32[]" = torch.ops.aten.div.Tensor(sum_3, convert_element_type_3);  sum_3 = None
        div_1: "f32[]" = torch.ops.aten.div.Tensor(tangents_1, convert_element_type_3);  tangents_1 = convert_element_type_3 = None
        unsqueeze_1: "i64[1024, 1]" = torch.ops.aten.unsqueeze.default(view_3, 1);  view_3 = None
        ne_3: "b8[1024, 1]" = torch.ops.aten.ne.Scalar(unsqueeze_1, -100)
        scalar_tensor_2: "i64[]" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.int64, layout = torch.strided, device = device(type='cuda', index=0))
        where_2: "i64[1024, 1]" = torch.ops.aten.where.self(ne_3, unsqueeze_1, scalar_tensor_2);  ne_3 = scalar_tensor_2 = None
        full: "f32[1024, 32000]" = torch.ops.aten.full.default([1024, 32000], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        scatter: "f32[1024, 32000]" = torch.ops.aten.scatter.value(full, 1, where_2, -1.0);  full = where_2 = None
        ne_4: "b8[1024, 1]" = torch.ops.aten.ne.Scalar(unsqueeze_1, -100);  unsqueeze_1 = None
        scalar_tensor_3: "f32[]" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0))
        where_3: "f32[1024, 1]" = torch.ops.aten.where.self(ne_4, div_1, scalar_tensor_3);  ne_4 = div_1 = scalar_tensor_3 = None
        mul: "f32[1024, 32000]" = torch.ops.aten.mul.Tensor(scatter, where_3);  scatter = where_3 = None
        alias_2: "f32[1024, 32000]" = torch.ops.aten.alias.default(alias_1);  alias_1 = None
        alias_3: "f32[1024, 32000]" = torch.ops.aten.alias.default(alias_2);  alias_2 = None
        exp_1: "f32[1024, 32000]" = torch.ops.aten.exp.default(alias_3);  alias_3 = None
        sum_4: "f32[1024, 1]" = torch.ops.aten.sum.dim_IntList(mul, [1], True)
        mul_1: "f32[1024, 32000]" = torch.ops.aten.mul.Tensor(exp_1, sum_4);  exp_1 = sum_4 = None
        sub_2: "f32[1024, 32000]" = torch.ops.aten.sub.Tensor(mul, mul_1);  mul = mul_1 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/loss/loss_utils.py:44 in ForCausalLMLoss, code: logits = logits.view(-1, vocab_size)
        view_4: "f32[1, 1024, 32000]" = torch.ops.aten.view.default(sub_2, [1, 1024, 32000]);  sub_2 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/loss/loss_utils.py:37 in ForCausalLMLoss, code: logits = logits.float()
        convert_element_type_4: "bf16[1, 1024, 32000]" = torch.ops.prims.convert_element_type.default(view_4, torch.bfloat16);  view_4 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/loss/loss_utils.py:37 in ForCausalLMLoss, code: logits = logits.float()
        add: "bf16[1, 1024, 32000]" = torch.ops.aten.add.Tensor(tangents_2, convert_element_type_4);  tangents_2 = convert_element_type_4 = None
        
         # File: /u/zwang22/desktop/transformers/src/transformers/models/llama/modeling_llama.py:2120 in torch_dynamo_resume_in_forward_at_2094, code: logits = self.lm_head(hidden_states[:, slice_indices, :])
        view_5: "bf16[1024, 32000]" = torch.ops.aten.view.default(add, [1024, 32000]);  add = None
        permute_1: "bf16[32000, 1024]" = torch.ops.aten.permute.default(view_5, [1, 0])
        mm_1: "bf16[32000, 4096]" = torch.ops.aten.mm.default(permute_1, view);  permute_1 = view = None
        permute_2: "bf16[4096, 32000]" = torch.ops.aten.permute.default(mm_1, [1, 0]);  mm_1 = None
        permute_3: "bf16[32000, 4096]" = torch.ops.aten.permute.default(permute, [1, 0]);  permute = None
        mm_2: "bf16[1024, 4096]" = torch.ops.aten.mm.default(view_5, permute_3);  view_5 = permute_3 = None
        view_6: "bf16[1, 1024, 4096]" = torch.ops.aten.view.default(mm_2, [1, 1024, 4096]);  mm_2 = None
        permute_4: "bf16[32000, 4096]" = torch.ops.aten.permute.default(permute_2, [1, 0]);  permute_2 = None
        full_1: "bf16[1, 1024, 4096]" = torch.ops.aten.full.default([1, 1024, 4096], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        slice_scatter: "bf16[1, 1024, 4096]" = torch.ops.aten.slice_scatter.default(full_1, view_6, 2, 0, 9223372036854775807);  full_1 = view_6 = None
        full_2: "bf16[1, 1024, 4096]" = torch.ops.aten.full.default([1, 1024, 4096], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        slice_scatter_1: "bf16[1, 1024, 4096]" = torch.ops.aten.slice_scatter.default(full_2, slice_scatter, 1, 0, 9223372036854775807);  full_2 = slice_scatter = None
        full_3: "bf16[1, 1024, 4096]" = torch.ops.aten.full.default([1, 1024, 4096], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        slice_scatter_2: "bf16[1, 1024, 4096]" = torch.ops.aten.slice_scatter.default(full_3, slice_scatter_1, 0, 0, 9223372036854775807);  full_3 = slice_scatter_1 = None
        return pytree.tree_unflatten([div, view_1, slice_scatter_2, None, permute_4], self._out_spec)